{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### To Use the Code in Google Colab\n",
    "\n",
    "To run the notebooks and access the data in Google Colab, follow these steps:\n",
    "\n",
    "1. **Create a Google account** if you don't already have one. This gives you access to both Colab and Google Drive.\n",
    "\n",
    "2. **Use the shared link** provided to you to copy (clone) the notebooks into your own Google Drive.\n",
    "\n",
    "3. **Download the corpora and exercise files** from the link provided, and upload them to your Google Drive.\n",
    "\n",
    "4. **Grant Colab access to your Google Drive** by running the following code:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "\n",
    "5. **You can now easily read files from your Google Drive** using standard file paths.\n",
    "\n",
    "---\n",
    "---"
   ],
   "id": "195b3f34e61edc09"
  },
  {
   "cell_type": "markdown",
   "id": "ff8e24c5-7b63-43a2-ab46-512d2f655561",
   "metadata": {},
   "source": "# Text and Its Features"
  },
  {
   "cell_type": "markdown",
   "id": "9a4258aa-cf1e-4484-a17a-a39ef6549740",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "8996640c-bc48-4c36-84de-0bd692421d60",
   "metadata": {},
   "source": [
    "## Exercise 1.1\n",
    "\n",
    "In this exercise, you will prepare your environment for text analysis using Python.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Upload files to Google Colab**\n",
    "   - Upload the provided Jupyter notebooks along with the required corpora and datasets (e.g., `.txt` files, `.csv` files) into your Google Colab environment.\n",
    "\n",
    "2. **Open a notebook**\n",
    "   - Open the exercise notebook where you will perform all tasks.\n",
    "\n",
    "3. **Import necessary Python packages**\n",
    "   - Import all libraries commonly used for text analysis, such as:\n",
    "     - `pandas`\n",
    "     - `nltk`\n",
    "     - `sklearn`\n",
    "     - `re`, `string`\n",
    "     - `matplotlib`, `seaborn` (for visualization, if needed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32d6f63-f53b-4447-95ca-09742dd1f784",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "38e4398a-6305-4a28-8043-fc222ca217c1",
   "metadata": {},
   "source": [
    "## Exercise 1.2\n",
    "\n",
    "In this exercise, you will load and combine two sources of data: the preambles of constitutions and metadata about countries.\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. The subfolder `'preamble'` in the exercise directory contains the **preambles of constitutions currently in force**, each stored as a separate text file.\n",
    "\n",
    "2. **Load these preambles into a DataFrame**\n",
    "   - Each row should represent one country or document.\n",
    "   - Include a column for the country code or file name, and another for the full text of the preamble.\n",
    "\n",
    "3. **Load the metadata CSV file**\n",
    "   - This file contains country-level information (e.g., region, legal system, population, etc.).\n",
    "\n",
    "4. **Merge the two DataFrames**\n",
    "   - Merge them on a common key (e.g., country code) to create one unified DataFrame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dc2644-e18d-44f3-807d-878e81436a47",
   "metadata": {},
   "source": ""
  },
  {
   "cell_type": "markdown",
   "id": "33bea447-cdac-4625-87ba-b7d5250cdf8f",
   "metadata": {},
   "source": [
    "## Exercise 1.3\n",
    "\n",
    "In this exercise, you will analyze the constitutional preambles loaded in the previous step.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Answer the following questions:**\n",
    "\n",
    "   1. üìù **Which country has the lengthiest preamble?**\n",
    "      - Use character count or word count to determine the length of each preamble.\n",
    "\n",
    "   2. üìö **Which country has the most difficult-to-read preamble?**\n",
    "      - Use a readability metric such as Flesch Reading Ease, Gunning Fog Index, or another suitable readability formula.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e880a74-0f30-40ee-9faf-b9263277f26f",
   "metadata": {},
   "source": [
    "## Exercise 1.4\n",
    "\n",
    "In this exercise, you will **tokenize** the preambles of constitutions.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Tokenize each document**\n",
    "   - Use a tokenizer from `nltk` (e.g., `nltk.word_tokenize`) or another reliable library.\n",
    "   - Tokenization means splitting each document into individual words or tokens.\n",
    "   - Store the tokenized version in a new column of your DataFrame.\n",
    "\n",
    "This step prepares the text for further processing such as stopword removal, lemmatization, or frequency analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ef974-a60c-404b-86ae-e39fa11a6ac6",
   "metadata": {},
   "source": [
    "## Exercise 1.5\n",
    "\n",
    "In this exercise, you will clean your tokenized documents by removing unwanted elements.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Remove stopwords and digits**\n",
    "   - Use a stopword list from `nltk.corpus.stopwords` or a custom list.\n",
    "   - Remove numeric tokens or any tokens that contain digits.\n",
    "\n",
    "2. **Define and remove domain-specific words**\n",
    "   - Identify a list of domain-specific or overused words that are not helpful for analysis (e.g., \"preamble\", \"constitution\", \"state\").\n",
    "   - Create a custom set of these words and remove them from your tokenized documents.\n",
    "\n",
    "The cleaned tokens will be more meaningful for later analysis steps such as frequency counting or sentiment analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df6dc70-14b4-44d1-b9ab-92454eaa5594",
   "metadata": {},
   "source": [
    "## Exercise 1.6\n",
    "\n",
    "In this exercise, you will apply **lemmatization** and **stemming** to your tokenized documents.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Lemmatization**\n",
    "   - Use a lemmatizer such as `WordNetLemmatizer` from `nltk.stem`.\n",
    "   - Lemmatization reduces words to their base or dictionary form (e.g., \"running\" ‚Üí \"run\").\n",
    "\n",
    "2. **Stemming**\n",
    "   - Use a stemmer such as `PorterStemmer` or `SnowballStemmer`.\n",
    "   - Stemming cuts off word suffixes to reduce them to a common root (e.g., \"governmental\" ‚Üí \"govern\").\n",
    "\n",
    "üëâ You can store the lemmatized and stemmed versions of your documents in separate columns to compare their effects."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "78551465d5c495dd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
