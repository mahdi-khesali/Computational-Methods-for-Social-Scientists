{
 "cells": [
  {
   "cell_type": "code",
   "id": "006212cd-97c2-4fbc-b511-54356e393f97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:11:19.226320Z",
     "start_time": "2025-07-27T09:11:19.220219Z"
    }
   },
   "source": [
    "from IPython.display import clear_output\n",
    "import time\n",
    "def countdown_timer(seconds):\n",
    "    for i in range(seconds, -1, -1):\n",
    "        clear_output(wait=True)\n",
    "        print(f\"⏳ Time remaining: {i} seconds\")\n",
    "        time.sleep(1)\n",
    "    print(\"✅ Time's up!\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "c8d6e645-cb93-42f0-bf6d-ae50cd649e9d",
   "metadata": {},
   "source": "# Session 1: Text and its features"
  },
  {
   "cell_type": "code",
   "id": "33d6a13f-1bf4-4a15-801f-4ac4afcb5279",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:11:34.537103Z",
     "start_time": "2025-07-27T09:11:21.076578Z"
    }
   },
   "source": [
    "import os  # Provides functions for interacting with the operating system (e.g., file paths, directories).\n",
    "import string  # Offers utilities for common string operations, such as punctuation handling.\n",
    "import pandas as pd  # Provides data structures and tools for data manipulation and analysis.\n",
    "import nltk  # A fundamental library for natural language processing tasks (e.g., tokenization, POS tagging).\n",
    "import readability  # Used to compute readability metrics and other linguistic features of text.\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "2b367627827aa25",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:11:34.951368Z",
     "start_time": "2025-07-27T09:11:34.548317Z"
    }
   },
   "source": [
    "# Download necessary NLTK resources for tokenization, lemmatization, and stopword removal\n",
    "nltk.download('punkt')        # Tokenizer models (used for splitting text into sentences or words)\n",
    "nltk.download('wordnet')      # WordNet lexical database (used for lemmatization)\n",
    "nltk.download('omw-1.4')      # Open Multilingual WordNet (supports lemmatization with multiple languages)\n",
    "nltk.download('stopwords')    # List of common stopwords in various languages"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to F:\\Mahdi\\Dropbox\\Dropbox\\code\n",
      "[nltk_data]     \\courseTeach\\.venv\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to F:\\Mahdi\\Dropbox\\Dropbox\\co\n",
      "[nltk_data]     de\\courseTeach\\.venv\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to F:\\Mahdi\\Dropbox\\Dropbox\\co\n",
      "[nltk_data]     de\\courseTeach\\.venv\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to F:\\Mahdi\\Dropbox\\Dropbox\\\n",
      "[nltk_data]     code\\courseTeach\\.venv\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "42e6de53bbdbe649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:11:34.974091Z",
     "start_time": "2025-07-27T09:11:34.969023Z"
    }
   },
   "source": [
    "# Initialize the stemmer and lemmatizer\n",
    "stemmer = nltk.stem.PorterStemmer()           # Reduces words to their stem (e.g., \"running\" → \"run\")\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()    # Reduces words to their base or dictionary form (lemma)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---"
   ],
   "id": "c9bd28fa111e68ca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1.1. Meta data",
   "id": "2639ff557c307f8f"
  },
  {
   "cell_type": "markdown",
   "id": "64438534-5622-42f9-ab56-0f69eb85e819",
   "metadata": {},
   "source": [
    "### Basic Features of Texts\n",
    "\n",
    "Each text contains three types of useful data:\n",
    "\n",
    "- **Metadata**\n",
    "  - Information about the text, such as the author, text type (e.g., speech, tweet), publication date, language, etc.\n",
    "  - Sometimes, metadata is embedded within the text itself.\n",
    "\n",
    "- **Textual Data**\n",
    "  - The linguistic features and actual content of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd1bfb-c654-454e-82b8-f2e4089a0037",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823ac0ed-12b1-479d-9df4-4e3a0de50b0f",
   "metadata": {},
   "source": [
    "### Metadata\n",
    "\n",
    "- When designing a text analysis project, think carefully about the types of information you want to collect.\n",
    "  - If certain metadata is not captured early on, recovering it later can be very time-consuming.\n",
    "\n",
    "- Metadata refers to information beyond the text itself and its linguistic features.\n",
    "  - Examples include the author, publication date, text type, language, and more.\n",
    "\n",
    "- There are three common ways to store and retrieve metadata:\n",
    "  - **Storing in a separate file** – Save metadata in a structured file (e.g., CSV or JSON) and load it when needed.\n",
    "  - **Encoding in the file name** – Embed metadata (e.g., date, author ID) directly in the file name and parse it during processing.\n",
    "  - **Using an API** – Retrieve metadata directly from online sources or databases."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---"
   ],
   "id": "876b0acbee8dce6f"
  },
  {
   "cell_type": "markdown",
   "id": "ff071cf2-b350-4e07-b340-4bdd965acb0a",
   "metadata": {},
   "source": "### Exercise 1.1"
  },
  {
   "cell_type": "code",
   "id": "715c04c2-6060-424f-8232-cf7b4b241aee",
   "metadata": {},
   "source": [
    "countdown_timer(300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "df5ec369-e98d-41fb-a18a-154a452c0ca0",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f72ffa-45ca-4bc3-8f04-943572132fc1",
   "metadata": {},
   "source": [
    "### Retrieve Metadata from File Names - Steps\n",
    "\n",
    "1. **Read the text files and store them in a dictionary.**\n",
    "\n",
    "2. **Convert the dictionary to a DataFrame.**\n",
    "   - One column should contain the file names.\n",
    "   - Another column should contain the corresponding text content.\n",
    "\n",
    "3. **Split the file names into multiple variables** (e.g., author, date, category), based on how you encoded the metadata in the file name.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 1 - Retrieve metadata from file names\n",
   "id": "b474865c1f3a2c8a"
  },
  {
   "cell_type": "code",
   "id": "e31173f8-2417-479f-a635-e4216b5954d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:12:00.050991Z",
     "start_time": "2025-07-27T09:11:57.436Z"
    }
   },
   "source": [
    "# Step 1: Read the files and store text and file names in a dictionary\n",
    "\n",
    "dictUNSpeech = {}  # Create an empty dictionary to store file name and text content\n",
    "\n",
    "# Define the directory containing the text files\n",
    "fileAddress1 = '../../corpusExample/unSpeeches2000_2010'\n",
    "\n",
    "# Loop through all files in the directory\n",
    "# For each file:\n",
    "#   - Open it\n",
    "#   - Read its content (converted to lowercase)\n",
    "#   - Store it in the dictionary with the filename (without '.txt') as the key\n",
    "for file in os.listdir(fileAddress1):\n",
    "    with open(os.path.join(fileAddress1, file), 'r', encoding='utf-8', errors='ignore') as textFile:\n",
    "        text = textFile.read().lower()\n",
    "        dictUNSpeech[file.replace('.txt', '')] = text\n",
    "\n",
    "# Step 2: Convert the dictionary to a DataFrame\n",
    "# The 'id' column will hold the cleaned file names, and 'text' will hold the corresponding content\n",
    "dfUNSpeech = pd.DataFrame(list(dictUNSpeech.items()), columns=[\"id\", \"text\"])\n",
    "\n",
    "# Step 3: Extract metadata from the file name\n",
    "# Each file name is assumed to follow the pattern: isoAlpha_session_year.txt (e.g., USA_65_2010)\n",
    "\n",
    "# Extract country code (isoAlpha), session number, and year from the file name\n",
    "dfUNSpeech[\"isoAlpha\"] = dfUNSpeech[\"id\"].str.split(\"_\", n=2, expand=True)[0].astype(str)\n",
    "dfUNSpeech[\"session\"] = dfUNSpeech[\"id\"].str.split(\"_\", n=2, expand=True)[1].astype(int)\n",
    "dfUNSpeech[\"year\"] = dfUNSpeech[\"id\"].str.split(\"_\", n=2, expand=True)[2].astype(int)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "fed1413f-63f6-4898-b85e-c1e001ed0d04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:12:00.708676Z",
     "start_time": "2025-07-27T09:12:00.677036Z"
    }
   },
   "source": [
    "dfUNSpeech.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                                               text isoAlpha  \\\n",
       "0  AFG_55_2000  on my way to the\\nassembly hall, i was informe...      AFG   \n",
       "1  AFG_56_2001  ﻿at the outset, on\\nbehalf of the government o...      AFG   \n",
       "2  AFG_57_2002  ﻿not very far from here stood\\ntwo towers that...      AFG   \n",
       "3  AFG_58_2003  ﻿there is no reality more\\noppressive than the...      AFG   \n",
       "4  AFG_59_2004  nelson mandela once\\ndescribed his countryís t...      AFG   \n",
       "\n",
       "   session  year  \n",
       "0       55  2000  \n",
       "1       56  2001  \n",
       "2       57  2002  \n",
       "3       58  2003  \n",
       "4       59  2004  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isoAlpha</th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>on my way to the\\nassembly hall, i was informe...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>55</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>﻿at the outset, on\\nbehalf of the government o...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>﻿not very far from here stood\\ntwo towers that...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>57</td>\n",
       "      <td>2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>﻿there is no reality more\\noppressive than the...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>58</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>nelson mandela once\\ndescribed his countryís t...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>59</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---"
   ],
   "id": "89141794e78a7071"
  },
  {
   "cell_type": "markdown",
   "id": "228594ee-127b-426d-8344-46936f69995f",
   "metadata": {},
   "source": [
    "### Example 2 - Retrieve Metadata from a Separate File — Steps\n",
    "\n",
    "1. **Read the dataset containing the metadata.**\n",
    "\n",
    "2. **Merge the metadata with the dataset containing the text files.**\n",
    "   - Use a common identifier (e.g., file name or country code) to perform the merge.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ea5f6d63-3165-452f-9670-e894eb16c9a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:14:17.174278Z",
     "start_time": "2025-07-27T09:14:15.573015Z"
    }
   },
   "source": [
    "fileAddress2 = '../../corpusExample/speakersSession.xlsx'\n",
    "speakerData = pd.read_excel(fileAddress2)\n",
    "dfUNSpeechComplete = dfUNSpeech.merge(speakerData, on=['year', 'isoAlpha'])"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "c49be6e3-3bab-4b30-ac22-eea3850f58bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:14:17.572130Z",
     "start_time": "2025-07-27T09:14:17.562357Z"
    }
   },
   "source": [
    "dfUNSpeechComplete.head(5)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                                               text isoAlpha  \\\n",
       "0  AFG_55_2000  on my way to the\\nassembly hall, i was informe...      AFG   \n",
       "1  AFG_56_2001  ﻿at the outset, on\\nbehalf of the government o...      AFG   \n",
       "2  AFG_57_2002  ﻿not very far from here stood\\ntwo towers that...      AFG   \n",
       "3  AFG_58_2003  ﻿there is no reality more\\noppressive than the...      AFG   \n",
       "4  AFG_59_2004  nelson mandela once\\ndescribed his countryís t...      AFG   \n",
       "\n",
       "   session  year  Session        cname        speakerName       post  \n",
       "0       55  2000       55  Afghanistan  Abdullah Abdullah        MFA  \n",
       "1       56  2001       56  Afghanistan      Ravan Farhâdi     UN_Rep  \n",
       "2       57  2002       57  Afghanistan       Hâmid Karzai  President  \n",
       "3       58  2003       58  Afghanistan       Hâmid Karzai  President  \n",
       "4       59  2004       59  Afghanistan   Mr. Hamid Karzai  President  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isoAlpha</th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>Session</th>\n",
       "      <th>cname</th>\n",
       "      <th>speakerName</th>\n",
       "      <th>post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>on my way to the\\nassembly hall, i was informe...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>55</td>\n",
       "      <td>2000</td>\n",
       "      <td>55</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Abdullah Abdullah</td>\n",
       "      <td>MFA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>﻿at the outset, on\\nbehalf of the government o...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56</td>\n",
       "      <td>2001</td>\n",
       "      <td>56</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Ravan Farhâdi</td>\n",
       "      <td>UN_Rep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>﻿not very far from here stood\\ntwo towers that...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>57</td>\n",
       "      <td>2002</td>\n",
       "      <td>57</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Hâmid Karzai</td>\n",
       "      <td>President</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>﻿there is no reality more\\noppressive than the...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>58</td>\n",
       "      <td>2003</td>\n",
       "      <td>58</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Hâmid Karzai</td>\n",
       "      <td>President</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>nelson mandela once\\ndescribed his countryís t...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>59</td>\n",
       "      <td>2004</td>\n",
       "      <td>59</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Mr. Hamid Karzai</td>\n",
       "      <td>President</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d83b4c46-4258-46d5-bd86-2cdf5c57253e",
   "metadata": {},
   "source": [
    "### Some Abbreviation\n",
    "- Prime Minister: PM \n",
    "- Deputy Prime Minister: DPM \n",
    "- Head of Government: HOG\n",
    "- Head of State: HOS\n",
    "- Minister for Foreign Affairs: MFA\n",
    "- UN Representative: UN_rep\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "---"
   ],
   "id": "c7271343b3acbc09"
  },
  {
   "cell_type": "markdown",
   "id": "b35e65cd-427d-4916-9a33-ec252a6faf02",
   "metadata": {},
   "source": [
    "### Importing a Corpus and Its Metadata via API: Steps\n",
    "\n",
    "1. **Create an account on the provider’s website.**\n",
    "\n",
    "2. **Obtain your API key.**\n",
    "   - The API key is a unique code that allows you to access your account programmatically through Python.\n",
    "\n",
    "3. **Read the API documentation** to understand the syntax and parameters for retrieving data.\n",
    "\n",
    "4. **Retrieve the data** using the appropriate request methods (e.g., `GET`, `POST`) and your API key.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Example 3 - working with API",
   "id": "f74163cd5a5d406"
  },
  {
   "cell_type": "markdown",
   "id": "e2e890f5-e80f-4af4-a844-87890bfdc674",
   "metadata": {},
   "source": [
    "Example 3: working with API**---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ab14ecf-b143-4ee6-9eb3-32233a62426f",
   "metadata": {},
   "source": [
    "### Exercise 1.2"
   ]
  },
  {
   "cell_type": "code",
   "id": "c3f83c7e-2d2c-4867-aa21-5f116de2eea1",
   "metadata": {},
   "source": [
    "countdown_timer(300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b7f5c741dab19afe",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23aff61-b44c-4c75-9a0b-e5b14ef1b774",
   "metadata": {},
   "source": [
    "## 1.2. Linguistic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72febbb8-d2c9-4644-9591-9de73cdc46c0",
   "metadata": {},
   "source": [
    "### How Do Linguistic Features Help Us?\n",
    "\n",
    "Sometimes, we don’t need complex methods to achieve our goals—simple measurements can be very informative.\n",
    "\n",
    "Examples of useful linguistic features:\n",
    "\n",
    "- Length of the text\n",
    "- Number of sentences\n",
    "- Average sentence length\n",
    "- Text complexity (e.g., readability)\n",
    "- Use of specific word types (e.g., adjectives, verbs)\n",
    "- Vocabulary richness\n",
    "- ...and more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd8beac7a16e1e8",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcdfa6baf5e26e",
   "metadata": {},
   "source": "### Example 1 - Measuring the length of documents"
  },
  {
   "cell_type": "code",
   "id": "c1a98b3a-dab5-4adf-9904-3ca8d594eb4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:21:23.834294Z",
     "start_time": "2025-07-27T09:21:23.612087Z"
    }
   },
   "source": [
    "# Calculate the number of words in each text\n",
    "# This splits each text into words (by whitespace) and counts the length of the resulting list\n",
    "dfUNSpeechComplete['word_count'] = dfUNSpeechComplete['text'].str.split().str.len()\n",
    "\n",
    "# Display the updated DataFrame\n",
    "dfUNSpeechComplete[['id', 'word_count']].head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "               id                                               text isoAlpha  \\\n",
       "0     AFG_55_2000  on my way to the\\nassembly hall, i was informe...      AFG   \n",
       "1     AFG_56_2001  ﻿at the outset, on\\nbehalf of the government o...      AFG   \n",
       "2     AFG_57_2002  ﻿not very far from here stood\\ntwo towers that...      AFG   \n",
       "3     AFG_58_2003  ﻿there is no reality more\\noppressive than the...      AFG   \n",
       "4     AFG_59_2004  nelson mandela once\\ndescribed his countryís t...      AFG   \n",
       "...           ...                                                ...      ...   \n",
       "2072  ZWE_61_2006  let me begin my statement \\nby echoing the sen...      ZWE   \n",
       "2073  ZWE_62_2007  allow me to congratulate \\nmr. kerim on his el...      ZWE   \n",
       "2074  ZWE_63_2008  i wish to begin by joining \\nthose who have co...      ZWE   \n",
       "2075  ZWE_64_2009  let me begin by extending \\nour warmest congra...      ZWE   \n",
       "2076  ZWE_65_2010  allow me once again to \\nextend to you, sir, o...      ZWE   \n",
       "\n",
       "      session  year  Session        cname                speakerName  \\\n",
       "0          55  2000       55  Afghanistan          Abdullah Abdullah   \n",
       "1          56  2001       56  Afghanistan              Ravan Farhâdi   \n",
       "2          57  2002       57  Afghanistan               Hâmid Karzai   \n",
       "3          58  2003       58  Afghanistan               Hâmid Karzai   \n",
       "4          59  2004       59  Afghanistan           Mr. Hamid Karzai   \n",
       "...       ...   ...      ...          ...                        ...   \n",
       "2072       61  2006       61     Zimbabwe  Mr. Robert Gabriel Mugabe   \n",
       "2073       62  2007       62     Zimbabwe           Robert G. Mugabe   \n",
       "2074       63  2008       63     Zimbabwe              Robert Mugabe   \n",
       "2075       64  2009       64     Zimbabwe           Robert G. Mugabe   \n",
       "2076       65  2010       65     Zimbabwe           Robert G. Mugabe   \n",
       "\n",
       "           post  word_count  \n",
       "0           MFA        2873  \n",
       "1        UN_Rep        2073  \n",
       "2     President        1700  \n",
       "3     President        1617  \n",
       "4     President        1096  \n",
       "...         ...         ...  \n",
       "2072  President        2371  \n",
       "2073  President        2052  \n",
       "2074  President        1800  \n",
       "2075  President        1722  \n",
       "2076  President        1558  \n",
       "\n",
       "[2077 rows x 10 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>isoAlpha</th>\n",
       "      <th>session</th>\n",
       "      <th>year</th>\n",
       "      <th>Session</th>\n",
       "      <th>cname</th>\n",
       "      <th>speakerName</th>\n",
       "      <th>post</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>on my way to the\\nassembly hall, i was informe...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>55</td>\n",
       "      <td>2000</td>\n",
       "      <td>55</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Abdullah Abdullah</td>\n",
       "      <td>MFA</td>\n",
       "      <td>2873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>﻿at the outset, on\\nbehalf of the government o...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>56</td>\n",
       "      <td>2001</td>\n",
       "      <td>56</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Ravan Farhâdi</td>\n",
       "      <td>UN_Rep</td>\n",
       "      <td>2073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>﻿not very far from here stood\\ntwo towers that...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>57</td>\n",
       "      <td>2002</td>\n",
       "      <td>57</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Hâmid Karzai</td>\n",
       "      <td>President</td>\n",
       "      <td>1700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>﻿there is no reality more\\noppressive than the...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>58</td>\n",
       "      <td>2003</td>\n",
       "      <td>58</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Hâmid Karzai</td>\n",
       "      <td>President</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>nelson mandela once\\ndescribed his countryís t...</td>\n",
       "      <td>AFG</td>\n",
       "      <td>59</td>\n",
       "      <td>2004</td>\n",
       "      <td>59</td>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>Mr. Hamid Karzai</td>\n",
       "      <td>President</td>\n",
       "      <td>1096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>ZWE_61_2006</td>\n",
       "      <td>let me begin my statement \\nby echoing the sen...</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>61</td>\n",
       "      <td>2006</td>\n",
       "      <td>61</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Mr. Robert Gabriel Mugabe</td>\n",
       "      <td>President</td>\n",
       "      <td>2371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>ZWE_62_2007</td>\n",
       "      <td>allow me to congratulate \\nmr. kerim on his el...</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>62</td>\n",
       "      <td>2007</td>\n",
       "      <td>62</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Robert G. Mugabe</td>\n",
       "      <td>President</td>\n",
       "      <td>2052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2074</th>\n",
       "      <td>ZWE_63_2008</td>\n",
       "      <td>i wish to begin by joining \\nthose who have co...</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>63</td>\n",
       "      <td>2008</td>\n",
       "      <td>63</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Robert Mugabe</td>\n",
       "      <td>President</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2075</th>\n",
       "      <td>ZWE_64_2009</td>\n",
       "      <td>let me begin by extending \\nour warmest congra...</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>64</td>\n",
       "      <td>2009</td>\n",
       "      <td>64</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Robert G. Mugabe</td>\n",
       "      <td>President</td>\n",
       "      <td>1722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2076</th>\n",
       "      <td>ZWE_65_2010</td>\n",
       "      <td>allow me once again to \\nextend to you, sir, o...</td>\n",
       "      <td>ZWE</td>\n",
       "      <td>65</td>\n",
       "      <td>2010</td>\n",
       "      <td>65</td>\n",
       "      <td>Zimbabwe</td>\n",
       "      <td>Robert G. Mugabe</td>\n",
       "      <td>President</td>\n",
       "      <td>1558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2077 rows × 10 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "8abb5cd8da584dc1",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521aeba8c942e793",
   "metadata": {},
   "source": "### Example 2 - Counting the number of sentences in documents"
  },
  {
   "cell_type": "code",
   "id": "cf46da03-ba4d-470b-894b-7e80bf3faae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:22:20.380191Z",
     "start_time": "2025-07-27T09:22:16.940198Z"
    }
   },
   "source": [
    "# Apply sentence tokenizer to count the number of sentences in each text\n",
    "# This uses NLTK's sentence tokenizer, which handles punctuation and abbreviation rules\n",
    "dfUNSpeechComplete['sentence_count'] = dfUNSpeechComplete['text'].apply(\n",
    "    lambda x: len(nltk.tokenize.sent_tokenize(x))\n",
    ")\n",
    "\n",
    "# Preview the result\n",
    "dfUNSpeechComplete[['id', 'sentence_count']].head()\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "b351f062e59a7a73",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b78ecd421e5009",
   "metadata": {},
   "source": "### Example 3 – Looking at the Frequency of Speeches from Different Officials"
  },
  {
   "cell_type": "code",
   "id": "4adebe77-81bc-4d9d-90be-3bf8a0f864c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:32:58.056565Z",
     "start_time": "2025-07-27T09:32:58.033464Z"
    }
   },
   "source": [
    "dfUNSpeechComplete['post'].value_counts()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "post\n",
       "MFA            955\n",
       "President      533\n",
       "PM             221\n",
       "UN_Rep         187\n",
       "DPM            109\n",
       "V-President     34\n",
       "HOS             24\n",
       "HOG             14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "3571b4c889773050",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0514e-84f4-4582-a4c1-021b46132af7",
   "metadata": {},
   "source": [
    "### Features of Text That Could Be Interesting\n",
    "\n",
    "When analyzing text, several features can provide valuable insights into structure, style, or complexity. Some commonly explored features include:\n",
    "\n",
    "- **Length** – Number of characters, words, or sentences\n",
    "- **Readability** – How easy or difficult the text is to understand (e.g., using Flesch-Kincaid score)\n",
    "- **Entropy** – A measure of unpredictability or information density in the text\n",
    "- **Lexical Diversity** – The variety of vocabulary used (e.g., type-token ratio)\n",
    "- **Verb Tense Usage** – Distribution of past, present, and future tenses, which can indicate narrative style or focus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c821628c-db2c-4af8-bdaa-ed4d7bf0df61",
   "metadata": {},
   "source": "### Example 4 – Linguistic Features: Readability Scores"
  },
  {
   "cell_type": "code",
   "id": "34363a7b-8cad-475c-adc0-10012bc8740d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:34:42.089411Z",
     "start_time": "2025-07-27T09:34:42.080039Z"
    }
   },
   "source": [
    "# Define a short example text\n",
    "SmallText = \"\"\"\n",
    "This is ILE summer school. It is held in Hamburg at the Institute for Law and Economics,\n",
    "which is one of its kind in Europe.\n",
    "\n",
    "This is the week of methods. We go through computational and experimental methods,\n",
    "which are common in Law and Economics research.\n",
    "\"\"\"\n",
    "\n",
    "# Extract various linguistic and readability features\n",
    "linguisticFeatures = readability.getmeasures(SmallText, lang='en')\n",
    "\n",
    "# Display the results\n",
    "linguisticFeatures\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('readability grades',\n",
       "              OrderedDict([('Kincaid', 6.550869565217393),\n",
       "                           ('ARI', 7.63644927536232),\n",
       "                           ('Coleman-Liau', 8.984565978260871),\n",
       "                           ('FleschReadingEase', 75.40644927536233),\n",
       "                           ('GunningFogIndex', 8.742028985507247),\n",
       "                           ('LIX', 41.42028985507246),\n",
       "                           ('SMOGIndex', 8.477225575051662),\n",
       "                           ('RIX', 4.0),\n",
       "                           ('DaleChallIndex', 3.5066202898550727)])),\n",
       "             ('sentence info',\n",
       "              OrderedDict([('characters_per_word', 4.543478260869565),\n",
       "                           ('syll_per_word', 1.3695652173913044),\n",
       "                           ('words_per_sentence', 15.333333333333334),\n",
       "                           ('sentences_per_paragraph', 1.5),\n",
       "                           ('type_token_ratio', 0.7391304347826086),\n",
       "                           ('directspeech_ratio', 0.0),\n",
       "                           ('characters', 209),\n",
       "                           ('syllables', 63),\n",
       "                           ('words', 46),\n",
       "                           ('wordtypes', 34),\n",
       "                           ('sentences', 3),\n",
       "                           ('paragraphs', 2),\n",
       "                           ('long_words', 12),\n",
       "                           ('complex_words', 3),\n",
       "                           ('complex_words_dc', 8)])),\n",
       "             ('word usage',\n",
       "              OrderedDict([('tobeverb', 5),\n",
       "                           ('auxverb', 0),\n",
       "                           ('conjunction', 3),\n",
       "                           ('pronoun', 5),\n",
       "                           ('preposition', 7),\n",
       "                           ('nominalization', 0)])),\n",
       "             ('sentence beginnings',\n",
       "              OrderedDict([('pronoun', 2),\n",
       "                           ('interrogative', 0),\n",
       "                           ('article', 0),\n",
       "                           ('subordination', 0),\n",
       "                           ('conjunction', 0),\n",
       "                           ('preposition', 0)]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "id": "a874d31a-f60d-455c-bf51-a2cfcd26d8e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:45:56.739003Z",
     "start_time": "2025-07-27T09:45:37.010187Z"
    }
   },
   "source": [
    "# Measure readability using Flesch Reading Ease score\n",
    "# The score indicates how easy a text is to read — higher values mean easier readability\n",
    "\n",
    "dfUNSpeechComplete['flesch_reading_ease'] = dfUNSpeechComplete['text'].apply(\n",
    "    lambda x: readability.getmeasures(x, lang='en')['readability grades']['FleschReadingEase']\n",
    ")\n",
    "\n",
    "# Display the updated DataFrame with readability scores\n",
    "dfUNSpeechComplete[['id', 'flesch_reading_ease']].head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id  flesch_reading_ease\n",
       "0  AFG_55_2000            47.500066\n",
       "1  AFG_56_2001            59.915970\n",
       "2  AFG_57_2002            53.515308\n",
       "3  AFG_58_2003            56.769041\n",
       "4  AFG_59_2004            52.270847"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>flesch_reading_ease</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>47.500066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>59.915970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>53.515308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>56.769041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>52.270847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "2f19c167cc3d883f",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816c04d65c91dbf",
   "metadata": {},
   "source": [
    "### Exercise 1.3"
   ]
  },
  {
   "cell_type": "code",
   "id": "39aedae03a3f2808",
   "metadata": {},
   "source": [
    "countdown_timer(300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "320aa5fa68d99e9",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Content of text",
   "id": "d050e88d0d3bba2d"
  },
  {
   "cell_type": "markdown",
   "id": "cc3fba223045b50d",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "To study the content of a text, we first break the document into smaller elements that can be measured and analyzed.\n",
    "These basic elements are called **tokens**.\n",
    "\n",
    "- A **token** is typically a word, but not always.\n",
    "- In most cases, a token corresponds to a word, but it can also be punctuation, a number, or even a sentence—depending on the type of tokenization.\n",
    "\n",
    "Tokenization is the first essential step in almost any text analysis task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4d9a199a3df308",
   "metadata": {},
   "source": [
    "### NLTK\n",
    "\n",
    "- In the earlier days of Python-based NLP (around 8 years ago), the **NLTK** package was the primary tool for performing basic natural language processing tasks.\n",
    "- While more modern libraries now exist (e.g., spaCy, transformers), NLTK is still widely used for teaching and for lightweight NLP tasks.\n",
    "\n",
    "In this example, we use NLTK to break down our documents into individual words (i.e., **tokenization**).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985baf66e7d216d",
   "metadata": {},
   "source": "### Example 1 - Tokenization"
  },
  {
   "cell_type": "code",
   "id": "c8c5fb9021671a8d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:45:23.488535Z",
     "start_time": "2025-07-27T09:45:11.854105Z"
    }
   },
   "source": [
    "# Tokenize the text column: split each document into a list of word tokens\n",
    "dfUNSpeechComplete['tokens'] = dfUNSpeechComplete['text'].apply(nltk.tokenize.word_tokenize)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "dfUNSpeechComplete[['id', 'tokens']].head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                                             tokens\n",
       "0  AFG_55_2000  [on, my, way, to, the, assembly, hall, ,, i, w...\n",
       "1  AFG_56_2001  [﻿at, the, outset, ,, on, behalf, of, the, gov...\n",
       "2  AFG_57_2002  [﻿not, very, far, from, here, stood, two, towe...\n",
       "3  AFG_58_2003  [﻿there, is, no, reality, more, oppressive, th...\n",
       "4  AFG_59_2004  [nelson, mandela, once, described, his, countr..."
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>[on, my, way, to, the, assembly, hall, ,, i, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>[﻿at, the, outset, ,, on, behalf, of, the, gov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>[﻿not, very, far, from, here, stood, two, towe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>[﻿there, is, no, reality, more, oppressive, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>[nelson, mandela, once, described, his, countr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "df8ec13fb2ce3e36",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c00253b0ba1e30",
   "metadata": {},
   "source": [
    "### Exersice 1.4"
   ]
  },
  {
   "cell_type": "code",
   "id": "14b9eb4b18d22ed7",
   "metadata": {},
   "source": [
    "countdown_timer(300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7f8e3ecbc7554c9f",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71983c96e2f0974b",
   "metadata": {},
   "source": [
    "### Removing Uninformative Tokens\n",
    "\n",
    "- Not all tokens are meaningful for text analysis.\n",
    "- We often want to remove tokens that do not carry substantial information.\n",
    "\n",
    "The most common examples include:\n",
    "\n",
    "- **Punctuation**\n",
    "- **Stopwords**: Common words such as pronouns, articles, and prepositions (e.g., \"he\", \"the\", \"in\") that occur frequently but add little analytical value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41719206e8dc4a3",
   "metadata": {},
   "source": "### Example 2 - removing words that are not meaningful"
  },
  {
   "cell_type": "code",
   "id": "ac6767d1e2155dc1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:46:57.301463Z",
     "start_time": "2025-07-27T09:46:57.276829Z"
    }
   },
   "source": [
    "# Load English stopwords from NLTK\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))  # Common words like \"the\", \"is\", \"in\", etc.\n",
    "\n",
    "# Load standard punctuation symbols from the string module\n",
    "punctuations = set(string.punctuation)  # Symbols like '.', ',', '!', '?', etc.\n"
   ],
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "id": "6a1ce8253f6e77e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:47:09.142708Z",
     "start_time": "2025-07-27T09:46:58.380278Z"
    }
   },
   "source": [
    "# Function to clean and tokenize text\n",
    "def text_cleaner(text):\n",
    "    # Tokenize the text into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # Convert to lowercase and remove stopwords and punctuation\n",
    "    clean_tokens = [\n",
    "        word.lower()\n",
    "        for word in tokens\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuations\n",
    "    ]\n",
    "\n",
    "    return clean_tokens\n",
    "\n",
    "# Apply the cleaning function to the text column\n",
    "dfUNSpeechComplete['tokens_clean'] = dfUNSpeechComplete['text'].apply(text_cleaner)\n"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "bc2f8c2f832b8e21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:47:12.189694Z",
     "start_time": "2025-07-27T09:47:12.183355Z"
    }
   },
   "source": [
    "# Access the first row and two specific variables (columns)\n",
    "dfUNSpeechComplete.iloc[0][['tokens', 'tokens_clean']]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens          [on, my, way, to, the, assembly, hall, ,, i, w...\n",
       "tokens_clean    [way, assembly, hall, informed, supreme, state...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "id": "988ce20f493015c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:48:07.693107Z",
     "start_time": "2025-07-27T09:48:07.686783Z"
    }
   },
   "source": [
    "# Compare the number of raw vs. clean tokens in the first document\n",
    "print(\"Length of original tokens:\", len(dfUNSpeechComplete.loc[0, 'tokens']))\n",
    "print(\"Length of cleaned tokens:\", len(dfUNSpeechComplete.loc[0, 'tokens_clean']))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of original tokens: 3171\n",
      "Length of cleaned tokens: 1622\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "84f2367845a129b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:48:29.973Z",
     "start_time": "2025-07-27T09:48:29.962431Z"
    }
   },
   "source": [
    "# Compare the number of unique tokens before and after cleaning\n",
    "print(\"Number of unique original tokens:\", len(set(dfUNSpeechComplete.loc[0, 'tokens'])))\n",
    "print(\"Number of unique cleaned tokens:\", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean'])))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique original tokens: 1023\n",
      "Number of unique cleaned tokens: 933\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "14339c679811d40e",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34184c6294a8593",
   "metadata": {},
   "source": [
    "### Exercise 1.5"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ae81b42a1c7df30",
   "metadata": {},
   "source": [
    "countdown_timer(300)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1b59185631bcd52a",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bf320a173cf145",
   "metadata": {},
   "source": [
    "### Words That Are Similar but Not Identical\n",
    "\n",
    "- Consider the words **run**, **running**, and **runs**:\n",
    "  - They have the same basic meaning.\n",
    "  - But from the perspective of a tokenizer, they are treated as different words.\n",
    "\n",
    "- This creates problems when we want to group similar forms of a word.\n",
    "\n",
    "- **Solutions:**\n",
    "  - **Stemming** – Reduces words to their root form by chopping off suffixes (e.g., \"running\" → \"run\").\n",
    "    - Often fast but may produce non-standard words.\n",
    "  - **Lemmatization** – Reduces words to their dictionary (lemma) form, using grammatical context.\n",
    "    - More accurate but requires more resources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b4a0682ccf18",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "- **Stemming** is a technique that reduces words to their root form by cutting off common suffixes.\n",
    "\n",
    "- For example:\n",
    "  - The words **run**, **running**, and **runs** are all reduced to **run**.\n",
    "  - This helps group similar words together and avoid counting them as separate tokens.\n",
    "\n",
    "- The result is that we get the same root word multiple times instead of treating each variation as different.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88b2d34b57bcdb",
   "metadata": {},
   "source": "### Example 3 - Stemming"
  },
  {
   "cell_type": "code",
   "id": "790eaf143f62d61e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:52:45.023452Z",
     "start_time": "2025-07-27T09:52:10.145223Z"
    }
   },
   "source": [
    "# Function to clean and stem tokens\n",
    "def text_cleaner(text):\n",
    "    # Step 1: Tokenize the text\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # Step 2: Lowercase and remove stopwords and punctuation\n",
    "    cleaned_tokens = [\n",
    "        word.lower()\n",
    "        for word in tokens\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuations\n",
    "    ]\n",
    "\n",
    "    # Step 3: Apply stemming to the cleaned tokens\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "\n",
    "    return cleaned_tokens, stemmed_tokens\n",
    "\n",
    "# Apply the function to each document and expand the result into two new columns\n",
    "dfUNSpeechComplete[['cleaned_tokens', 'tokens_clean_stemmed']] = (\n",
    "    dfUNSpeechComplete['text']\n",
    "    .apply(text_cleaner)\n",
    "    .apply(pd.Series)\n",
    ")\n",
    "\n",
    "# Preview the result\n",
    "dfUNSpeechComplete[['id', 'cleaned_tokens', 'tokens_clean_stemmed']].head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                                     cleaned_tokens  \\\n",
       "0  AFG_55_2000  [way, assembly, hall, informed, supreme, state...   \n",
       "1  AFG_56_2001  [﻿at, outset, behalf, government, islamic, sta...   \n",
       "2  AFG_57_2002  [﻿not, far, stood, two, towers, symbolized, fr...   \n",
       "3  AFG_58_2003  [﻿there, reality, oppressive, silence, nation,...   \n",
       "4  AFG_59_2004  [nelson, mandela, described, countryís, transi...   \n",
       "\n",
       "                                tokens_clean_stemmed  \n",
       "0  [way, assembl, hall, inform, suprem, state, co...  \n",
       "1  [﻿at, outset, behalf, govern, islam, state, af...  \n",
       "2  [﻿not, far, stood, two, tower, symbol, freedom...  \n",
       "3  [﻿there, realiti, oppress, silenc, nation, lon...  \n",
       "4  [nelson, mandela, describ, countryí, transit, ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>tokens_clean_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>[way, assembly, hall, informed, supreme, state...</td>\n",
       "      <td>[way, assembl, hall, inform, suprem, state, co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>[﻿at, outset, behalf, government, islamic, sta...</td>\n",
       "      <td>[﻿at, outset, behalf, govern, islam, state, af...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>[﻿not, far, stood, two, towers, symbolized, fr...</td>\n",
       "      <td>[﻿not, far, stood, two, tower, symbol, freedom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>[﻿there, reality, oppressive, silence, nation,...</td>\n",
       "      <td>[﻿there, realiti, oppress, silenc, nation, lon...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>[nelson, mandela, described, countryís, transi...</td>\n",
       "      <td>[nelson, mandela, describ, countryí, transit, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "id": "2d14f06981730091",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:52:45.079722Z",
     "start_time": "2025-07-27T09:52:45.069020Z"
    }
   },
   "source": [
    "# Access the first row and select specific columns: original, cleaned, and stemmed tokens\n",
    "dfUNSpeechComplete.loc[0, ['tokens', 'tokens_clean', 'tokens_clean_stemmed']]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens                  [on, my, way, to, the, assembly, hall, ,, i, w...\n",
       "tokens_clean            [way, assembly, hall, informed, supreme, state...\n",
       "tokens_clean_stemmed    [way, assembl, hall, inform, suprem, state, co...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "id": "fbaadca1468ac058",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:52:48.642726Z",
     "start_time": "2025-07-27T09:52:48.636405Z"
    }
   },
   "source": [
    "# Compare the number of unique tokens before and after cleaning and stemming\n",
    "print(\"Number of unique original tokens:       \", len(set(dfUNSpeechComplete.loc[0, 'tokens'])))\n",
    "print(\"Number of unique cleaned tokens:        \", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean'])))\n",
    "print(\"Number of unique cleaned & stemmed tokens:\", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean_stemmed'])))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique original tokens:        1023\n",
      "Number of unique cleaned tokens:         933\n",
      "Number of unique cleaned & stemmed tokens: 819\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "83805c3f4c376aad",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "- Lemmatization goes one step further than stemming.\n",
    "- It transforms a word into its **dictionary base form** (known as the *lemma*).\n",
    "- Unlike stemming, lemmatization considers the context and part of speech to produce real words.\n",
    "\n",
    "For example:\n",
    "- \"running\", \"ran\" → \"run\"\n",
    "- \"better\" → \"good\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c14baec0c11ab2",
   "metadata": {},
   "source": "### Example 4 - Lemmatization"
  },
  {
   "cell_type": "code",
   "id": "2a36c8945c21f785",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:55:08.438788Z",
     "start_time": "2025-07-27T09:54:21.603956Z"
    }
   },
   "source": [
    "# Function to clean, stem, and lemmatize tokens\n",
    "def text_cleaner(text):\n",
    "    # Step 1: Tokenize the text into words\n",
    "    tokens = nltk.tokenize.word_tokenize(text)\n",
    "\n",
    "    # Step 2: Lowercase and remove stopwords and punctuation\n",
    "    cleaned_tokens = [\n",
    "        word.lower()\n",
    "        for word in tokens\n",
    "        if word.lower() not in stop_words and word.lower() not in punctuations\n",
    "    ]\n",
    "\n",
    "    # Step 3: Apply stemming\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in cleaned_tokens]\n",
    "\n",
    "    # Step 4: Apply lemmatization\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in cleaned_tokens]\n",
    "\n",
    "    return cleaned_tokens, stemmed_tokens, lemmatized_tokens\n",
    "\n",
    "# Apply the function and expand the results into separate columns\n",
    "dfUNSpeechComplete[['cleaned_tokens', 'tokens_clean_stemmed', 'tokens_clean_lemmatized']] = (\n",
    "    dfUNSpeechComplete['text'].apply(text_cleaner).apply(pd.Series)\n",
    ")\n",
    "\n",
    "# Preview the result\n",
    "dfUNSpeechComplete[['id', 'cleaned_tokens', 'tokens_clean_stemmed', 'tokens_clean_lemmatized']].head()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            id                                     cleaned_tokens  \\\n",
       "0  AFG_55_2000  [way, assembly, hall, informed, supreme, state...   \n",
       "1  AFG_56_2001  [﻿at, outset, behalf, government, islamic, sta...   \n",
       "2  AFG_57_2002  [﻿not, far, stood, two, towers, symbolized, fr...   \n",
       "3  AFG_58_2003  [﻿there, reality, oppressive, silence, nation,...   \n",
       "4  AFG_59_2004  [nelson, mandela, described, countryís, transi...   \n",
       "\n",
       "                                tokens_clean_stemmed  \\\n",
       "0  [way, assembl, hall, inform, suprem, state, co...   \n",
       "1  [﻿at, outset, behalf, govern, islam, state, af...   \n",
       "2  [﻿not, far, stood, two, tower, symbol, freedom...   \n",
       "3  [﻿there, realiti, oppress, silenc, nation, lon...   \n",
       "4  [nelson, mandela, describ, countryí, transit, ...   \n",
       "\n",
       "                             tokens_clean_lemmatized  \n",
       "0  [way, assembly, hall, informed, supreme, state...  \n",
       "1  [﻿at, outset, behalf, government, islamic, sta...  \n",
       "2  [﻿not, far, stood, two, tower, symbolized, fre...  \n",
       "3  [﻿there, reality, oppressive, silence, nation,...  \n",
       "4  [nelson, mandela, described, countryís, transi...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "      <th>tokens_clean_stemmed</th>\n",
       "      <th>tokens_clean_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AFG_55_2000</td>\n",
       "      <td>[way, assembly, hall, informed, supreme, state...</td>\n",
       "      <td>[way, assembl, hall, inform, suprem, state, co...</td>\n",
       "      <td>[way, assembly, hall, informed, supreme, state...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AFG_56_2001</td>\n",
       "      <td>[﻿at, outset, behalf, government, islamic, sta...</td>\n",
       "      <td>[﻿at, outset, behalf, govern, islam, state, af...</td>\n",
       "      <td>[﻿at, outset, behalf, government, islamic, sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AFG_57_2002</td>\n",
       "      <td>[﻿not, far, stood, two, towers, symbolized, fr...</td>\n",
       "      <td>[﻿not, far, stood, two, tower, symbol, freedom...</td>\n",
       "      <td>[﻿not, far, stood, two, tower, symbolized, fre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AFG_58_2003</td>\n",
       "      <td>[﻿there, reality, oppressive, silence, nation,...</td>\n",
       "      <td>[﻿there, realiti, oppress, silenc, nation, lon...</td>\n",
       "      <td>[﻿there, reality, oppressive, silence, nation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AFG_59_2004</td>\n",
       "      <td>[nelson, mandela, described, countryís, transi...</td>\n",
       "      <td>[nelson, mandela, describ, countryí, transit, ...</td>\n",
       "      <td>[nelson, mandela, described, countryís, transi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "9c91a275d0793292",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:55:08.491164Z",
     "start_time": "2025-07-27T09:55:08.481416Z"
    }
   },
   "source": [
    "# Access the first row and view the original, cleaned, stemmed, and lemmatized tokens\n",
    "dfUNSpeechComplete.loc[0, ['tokens', 'tokens_clean', 'tokens_clean_stemmed', 'tokens_clean_lemmatized']]\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokens                     [on, my, way, to, the, assembly, hall, ,, i, w...\n",
       "tokens_clean               [way, assembly, hall, informed, supreme, state...\n",
       "tokens_clean_stemmed       [way, assembl, hall, inform, suprem, state, co...\n",
       "tokens_clean_lemmatized    [way, assembly, hall, informed, supreme, state...\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "code",
   "id": "5a0d1c1b6f1b5e31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T09:55:08.594154Z",
     "start_time": "2025-07-27T09:55:08.586028Z"
    }
   },
   "source": [
    "# Compare the number of unique tokens at different stages of preprocessing\n",
    "print(\"Number of unique original tokens:         \", len(set(dfUNSpeechComplete.loc[0, 'tokens'])))\n",
    "print(\"Number of unique cleaned tokens:          \", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean'])))\n",
    "print(\"Number of unique cleaned + stemmed tokens:\", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean_stemmed'])))\n",
    "print(\"Number of unique cleaned + lemmatized tokens:\", len(set(dfUNSpeechComplete.loc[0, 'tokens_clean_lemmatized'])))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique original tokens:          1023\n",
      "Number of unique cleaned tokens:           933\n",
      "Number of unique cleaned + stemmed tokens: 819\n",
      "Number of unique cleaned + lemmatized tokens: 898\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "id": "870bc01c1b86862f",
   "metadata": {},
   "source": [
    "### Exercise 1.6"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
