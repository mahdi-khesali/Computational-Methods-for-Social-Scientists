{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### To Use the Code in Google Colab\n",
    "\n",
    "To run the notebooks and access the data in Google Colab, follow these steps:\n",
    "\n",
    "1. **Create a Google account** if you don't already have one. This gives you access to both Colab and Google Drive.\n",
    "\n",
    "2. **Use the shared link** provided to you to copy (clone) the notebooks into your own Google Drive.\n",
    "\n",
    "3. **Download the corpora and exercise files** from the link provided, and upload them to your Google Drive.\n",
    "\n",
    "4. **Grant Colab access to your Google Drive** by running the following code:\n",
    "\n",
    "```python\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "\n",
    "5. **You can now easily read files from your Google Drive** using standard file paths.\n",
    "\n",
    "---\n",
    "---"
   ],
   "id": "d95f64aa231df68f"
  },
  {
   "cell_type": "markdown",
   "id": "0aee9e3f-21ca-4142-8304-a02627b5b7f5",
   "metadata": {},
   "source": [
    "# Exercises for session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf848827-1cd8-4882-9747-9f86725acfe7",
   "metadata": {},
   "source": [
    "## Exercise 2.1\n",
    "\n",
    "In this exercise, you will build a document-frequency matrix from a collection of text files.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Read the corpus**\n",
    "   - Load all text files from the specified folder.\n",
    "\n",
    "2. **Create a DataFrame**\n",
    "   - Each row should represent one document.\n",
    "   - Include a variable (column) that stores the full text of each document.\n",
    "\n",
    "3. **Create a Document-Frequency Matrix (DFM)**\n",
    "   - Use a vectorizer (e.g., `CountVectorizer`) to convert the text into numerical features.\n",
    "   - Each row should represent a document, and each column a word.\n",
    "   - The values should reflect how often each word appears in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a24e0e-7d47-49f8-9427-576fd5747cc7",
   "metadata": {},
   "source": [
    "## Exercise 2.2\n",
    "\n",
    "In this exercise, you will build your own custom **text preprocessor** and pass it to `CountVectorizer()`.\n",
    "\n",
    "### Tasks:\n",
    "\n",
    "1. **Write a custom preprocessing function in Python.**\n",
    "   - This function will take a raw text string as input and return a list of cleaned tokens.\n",
    "\n",
    "2. **The function should include the following steps:**\n",
    "   - âœ… Remove stopwords\n",
    "   - âœ… Remove punctuation\n",
    "   - âœ… Tokenize using **NLTK's tokenizer**\n",
    "   - âœ… Apply **lemmatization** to each token\n",
    "\n",
    "3. **Use your custom function with `CountVectorizer()`**\n",
    "   - Pass it via the `analyzer` argument to create a document-term matrix.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06d8ed6-ff49-4d4b-b7f1-2603e7eadee5",
   "metadata": {},
   "source": [
    "## Exercise 2.3\n",
    "\n",
    "In this exercise, you will convert raw text into a **Term Frequency (TF) matrix**.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Convert the text into a Term Frequency matrix**\n",
    "   - Use `CountVectorizer()` or `TfidfVectorizer(use_idf=False)` to compute **normalized term frequencies**.\n",
    "   - Make sure your vectorizer uses the custom preprocessor you built in the previous exercise.\n",
    "   - The resulting matrix should contain **relative term frequencies** (not raw counts).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce78b6d-f155-4ddb-993f-3ed77805c986",
   "metadata": {},
   "source": [
    "## Exercise 2.4\n",
    "\n",
    "In this exercise, you will convert your corpus into a **TF-IDF matrix**.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Convert the texts to a TF-IDF matrix**\n",
    "   - Use `TfidfVectorizer()` from `sklearn.feature_extraction.text`.\n",
    "   - Make sure to pass your custom preprocessor (from Exercise 2.2) via the `analyzer` argument.\n",
    "   - This matrix will reflect both:\n",
    "     - **Term Frequency** (how often a word appears in a document)\n",
    "     - **Inverse Document Frequency** (how unique the word is across all documents)\n",
    "\n",
    "The result will be a matrix where each value represents how important a word is in a specific document, relative to the entire corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c7b049-c82a-4f12-b073-df1e8c4eabf5",
   "metadata": {},
   "source": [
    "## Exercise 2.5\n",
    "\n",
    "In this exercise, you will practice reading and explaining code by writing clear and helpful comments.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Write comments on a given piece of code**\n",
    "   - For each major step or line, explain **what** the code does and **why** it is needed.\n",
    "   - Use complete sentences or clear phrases.\n",
    "   - Write comments in a way that someone new to text processing could understand.\n",
    "\n",
    "This exercise will help you build a deeper understanding of the logic behind text preprocessing and matrix construction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2ddb82-5488-4474-bd62-19d44f559e40",
   "metadata": {},
   "source": [
    "## Exercise 2.6\n",
    "\n",
    "In this exercise, you will perform **sentiment analysis** on the **preambles of constitutions**.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Analyze the sentiment of each preamble**\n",
    "   - Use a sentiment analysis tool such as **TextBlob** or **VADER**.\n",
    "   - Apply it to the full text of each preamble to calculate sentiment scores (e.g., polarity).\n",
    "\n",
    "2. **Answer the following questions:**\n",
    "   - ðŸŸ¢ **Which preamble has the highest sentiment score?**\n",
    "   - ðŸ”´ **Which preamble has the lowest sentiment score?**\n",
    "\n",
    "Make sure to include both the **country name** (or document ID) and the **sentiment score** in your answers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7918c1-8322-4d70-a831-1eb558210435",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "In his paper *Constitutional Archetypes*, constitutional law scholar **David Law** argues that constitutional preambles can be grouped into three main categories:\n",
    "\n",
    "- **Liberal**\n",
    "- **Statist**\n",
    "- **Universal**\n",
    "\n",
    "Law provides linguistic and conceptual evidence supporting these three archetypes based on word choice and framing.\n",
    "\n",
    "### Task:\n",
    "\n",
    "1. **Read the paper by David Law carefully**\n",
    "   - Focus especially on the sections that define and describe the three preamble types.\n",
    "\n",
    "2. **Create dictionaries for each archetype**\n",
    "   - Based on the paper, build a **dictionary of representative words** for each category:\n",
    "     - Liberal\n",
    "     - Statist\n",
    "     - Universal\n",
    "\n",
    "3. **Apply the dictionary approach**\n",
    "   - Write Python code that scans each preamble and calculates the number (or proportion) of words matching each dictionary.\n",
    "   - Assign or score each preamble according to how strongly it aligns with each archetype.\n",
    "\n",
    "4. **Optional:**\n",
    "   - Visualize the distribution of preamble types across countries or regions.\n",
    "   - Explore if certain types are more common in specific eras or continents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
