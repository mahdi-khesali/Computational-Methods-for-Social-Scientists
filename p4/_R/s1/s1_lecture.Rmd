---
title: "Session 1"
author: "Mahdi"
date: "2025-05-20"
output: ioslides_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r install & load packages, include=FALSE}
# Install packages that are required for text analysis in R
# install.packages("readtext", repos = "https://cloud.r-project.org") # Reading text files
# install.packages("tm", repos = "https://cloud.r-project.org") # Text mining
# install.packages("SnowballC", repos = "https://cloud.r-project.org") # Word stemming
# install.packages("wordcloud", repos = "https://cloud.r-project.org") # Word clouds
# install.packages("textclean", repos = "https://cloud.r-project.org") # Cleaning text
# install.packages("textdata", repos = "https://cloud.r-project.org") # Pretrained sentiment lexicons
# install.packages("tidytext", repos = "https://cloud.r-project.org") # Tidy text analysis
# install.packages("tokenizers", repos = "https://cloud.r-project.org") # Tokenizing text
# install.packages("quanteda", repos = "https://cloud.r-project.org") # Main package for text analysis in R
# install.packages("quanteda.textmodels", repos = "https://cloud.r-project.org") # using spaCy interface
# install.packages("quanteda.textstats", repos = "https://cloud.r-project.org") # using spaCy interface
# install.packages("quanteda.textplots", repos = "https://cloud.r-project.org") # using spaCy interface
# install.packages("stringr", repos = "https://cloud.r-project.org") # String manipulation
# install.packages("text", repos = "https://cloud.r-project.org")  # Embedding models
# install.packages("syuzhet", repos = "https://cloud.r-project.org") # Sentiment analysis
# install.packages("topicmodels", repos = "https://cloud.r-project.org")  # Topic modeling
# install.packages("ggplot2", repos = "https://cloud.r-project.org") # Visualization
# install.packages("spacyr", repos = "https://cloud.r-project.org") # using spaCy interface
# install.packages("textstem", repos = "https://cloud.r-project.org") # using spaCy interface

# other packages 
# install.packages("countdown", repos = "https://cloud.r-project.org") 
# install.packages("readxl", repos = "https://cloud.r-project.org") 
# install.packages("tidyverse", repos = "https://cloud.r-project.org") 

```


```{r load all required packages, include=FALSE}
# Load required packages
library("readtext")
library("tm")
library("SnowballC")
library("wordcloud")
library("textclean")
library("textdata")
library("tidytext")
library("tokenizers")
library("quanteda")
library("quanteda.textmodels")
library("quanteda.textplots")
library("quanteda.textstats")
library("textstem")
library("stringr")
library("syuzhet")
library("topicmodels")
library("ggplot2")
###
library("countdown")
library("readxl")
library("tidyr")
library("dplyr")
library("tidyverse")
library(DiagrammeR)
```

## The basic idea 

```{r}
DiagrammeR::grViz("
digraph flowchart {
  graph [layout = dot, rankdir = LR]

  node [shape = box, style = filled, color = lightblue]

  A [label = 'External phenomena']
  B [label = 'Content']
  C [label = 'Linguistic features']

  A -> {B C}
}
")


```



## A very first step 
You need to set a directory where all files are. 

```{r set the directory}
# make sure that you set the directory correctly
setwd("C:/Users/bax1408/Dropbox/code/courseTeach/computationalSocialScience/_R/s1")
# setwd("F:/Mahdi/Dropbox/Dropbox/code/courseTeach/computationalSocialScience/_R/s1")
```

Let us check the directory!

```{r check the directory}
# make sure that you set the directory correctly
print(getwd())
```



## Text and Its Features

## Part 1: Text Metadata

Each text has three types of data that can be useful: 

- Metadata 
  - Any information about a text: author, type of text (speech, tweet,...), date of publication, language, etc.
    - Sometimes metadata appears in text itself. 
    
    
- Textual data 
  - Linguistic features of text
  - Content of text 
  
## Metadata

When designing a project containing text analysis think carefully what kind of information you want to gather:

  - You have to spend a huge amount of time to recover information that you did not code. 

3 ways to store and retrieve metadata

  - Working with API
  - Store in separate file and retrieve from it.  
  - Store in the name of file and retrieve from it. 
  


## Exercise 1.1



```{r countdown, echo=FALSE}
countdown(minutes = 0, seconds = 15)
```



## Example 3.1: Retreive from File Name 
**Step 1: read the files**
```{r retreive files}
# The directory
fileAddress1 <- '../../corpusExample/unSpeeches2000_2010'

# Read all .txt files in the directory
dfUNSpeech <- readtext(paste0(fileAddress1, "/*.txt"), 
                           docvarsfrom = "filenames",
                           docvarnames = c("isoAlpha", "session", "year"),
                           dvsep = "_",
                           encoding = "ISO-8859-1")
```

## Example 2: Retreieve from Separate File 

```{r}
# Read your file 
fileAddress2 = '../../corpusExample/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)

dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
```

## Some Abbreviation 
- Prime Minister: PM 
- Deputy Prime Minister: DPM 
- Head of Government: HOG
- Head of State: HOS
- Minister for Foreign Affairs: MFA
-UN Representative: UN_rep


## Example 1: Retrieve from API 

```{r retrieve API}

```



## Example 3.3

```{r check the data}
head(dfUNSpeech)
```

## Types of documents that you can read into R using readtext
- Docx
- JSON
- XML
- Zip files


## Exercise 1.2



```{r countdown, echo=FALSE}
countdown(minutes = 0, seconds = 15)
```




## Part 2: Linguistic Features

## How Does Linguistic Features help us? 
Sometimes we do not need a complex method to achieve our goal. 

Simple measurements are useful. 

Some linguistic features:
- Length of text
- Number of sentences 
- Length of sentences 
- Complexity of text
- ... 



## preparing our corpus
We need to prepare our corpus in R. 
  - Raw text -> corpus -> token
  - Quanteda has data type called corpus that makes our life much easier. 
  - This is different in the classical NLP packages such as NLTK in Python. 


## Define Corpus 
```{r defining corpus}
corpusUNSpeechComplete <- corpus(dfUNSpeechComplete) 
# feed the dataframe that you read by readtext command. 
# The dataframe should be created by readtext only. 
```


```{r}
print(corpusUNSpeechComplete)
```

## Looking at summary 

```{r}
# use summary() to check the basic linguistic features of document 
summary(corpusUNSpeechComplete, 20)
```


## A better overview 
Use textstat_summary()

```{r}
textstat_summary(corpusUNSpeechComplete)
```



## Exercise 1.3



```{r countdown, echo=FALSE}
countdown(minutes = 0, seconds = 15)
```



## Check the frequency of document in each group

```{r}
table(corpusUNSpeechComplete$post)
```


## Checking each document

```{r}
print(as.character(corpusUNSpeechComplete[1]))
```

## Features of Text that could be interesting
- Length 
- Readability 
- Entropy
- Lexical diversity
- Tenses of verbs 



## complexity of text 

```{r}
epcomplexity <- textstat_readability(corpusUNSpeechComplete, c("Flesch","LIW"))
epcomplexity <- data.frame(epcomplexity,post=corpusUNSpeechComplete$post)
```


```{r}
epcomplexity %>% 
  ggplot(aes(x=fct_reorder(post,Flesch), y=Flesch,fill=post)) + 
  geom_boxplot() + 
  geom_jitter(position=position_jitter(width=.1, height=0))+
  theme_bw()+
  labs(x="",y="Flesch Reading Ease Score")+
  theme(legend.position = "none")
```


## Part 3: Linguistic Features Part 2 




## Content of Text 


## Tokenizing: Description

Tokenization means conversion of texts to words. 

Quanteda still preserves order of words in this stage. 


## Tokenizing: Example 

```{r}
tokenPreamble <- tokens(corpusPreamble)
tokenPreamble[1]
```



## Remove Stopwords 
**Stopwords:** any word that does not contribute to the meaning of a text. 

We remove stopwords to achieve less complexity and less dimensionality. 


**Examples:**
- General
  - Punctuation 
  - Articles (a, the)
  - Pronomens 
  - 
- Domain-specific 
  - 

## Stopwords Removed Examples I 

```{r}
# Remove general stopwords 
tokenPreambleClean <- tokens(
  corpusPreamble, 
  remove_numbers = TRUE,
  remove_punct = TRUE, 
  remove_symbols = TRUE,
  remove_url = TRUE
) %>% 
  tokens_remove(stopwords("english"))
tokenPreambleClean[1]
```



## Stopwords Removed Examples II

- Identify a bad practice here. 

```{r}
# Remove Domain-specific stopwords 
tokenPreambleClean <- 
```



## Capitalization  
Something that we usually forget, because it is a part of our common sense. 

In python you need to deal with the problem yourself. ´Quanteda´takes care of it. 

```{r}
exampleCapital <- c(sentenceExample1 = "Capital letters create a challenge for us. 
                   The Challenge is that computer treats same words written in 
                   capital letter differently from the same word written in lower cases",
                   sentenceExample2 = "capital letters create a challenge for us. 
                   the challenge is that computer treats same words written in 
                   capital letter differently from the same word written in lower cases")
capitalMatrix <-  corpus(exampleCapital) %>% 
                 tokens() %>% 
                 dfm() %>% 
                 convert(to= "data.frame")
capitalMatrix
```


## Stemming 

```{r}
tokenPreambleCleanStemmed <- tokenPreambleClean %>%
  tokens_wordstem(language = "english")
```


## Lemmatization 

```{r}
# Lemmatize the character tokens
lemmatized <- lemmatize_words(as.character(tokenPreambleClean))

```



```{r}

```










