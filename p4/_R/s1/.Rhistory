patience_HHI,
punctuality_HHI
from
vars_histoVasHetero_phase2"
df_workEthics <- dbGetQuery(conn, query_workEthics)
df_workEthics_z <- as.data.frame(scale(df_workEthics))
query_entre <- "select
socialStatus_HHI,
geoMobile_HHI,
riskReasonable_HHI,
restart_HHI
from
vars_histoVasHetero_phase2"
df_entre <- dbGetQuery(conn, query_entre)
df_entre_z <- as.data.frame(scale(df_entre))
query_acceptEntre <- "select
individualism_HHI,
differentOutcome_HHI,
diversityAccept_HHI,
changeOpen_HHI,
envy_HHI
from
vars_histoVasHetero_phase2"
df_acceptEntre <- dbGetQuery(conn, query_acceptEntre)
df_acceptEntre_z <- as.data.frame(scale(df_acceptEntre))
query_absDiscrimination <- "select
firstImpress_HHI,
femaleSaved_HHI,
elderlyTreat_HHI,
discriminationEnco_HHI,
discriminationFemaleEnco_HHI
from
vars_histoVasHetero_phase2"
df_absDiscrimination <- dbGetQuery(conn, query_absDiscrimination)
df_absDiscrimination_z <- as.data.frame(scale(df_absDiscrimination))
query_challengeAuthority <- "select
normAbidance_HHI,
ChallengeAuthority_HHI,
challengedAuthorityType_HHI
from
vars_histoVasHetero_phase2
"
df_challengeAuthority <- dbGetQuery(conn, query_challengeAuthority)
df_challengeAuthority_z <- as.data.frame(scale(df_challengeAuthority))
fa_cooperation <- factanal(df_cooperation_z, factors = 1, scores = "regression")
fa_altruism <- factanal(df_altruism_z, factors = 1, scores = "regression")
fa_workEthics <- factanal(df_workEthics_z, factors = 1, scores = "regression")
fa_entre <- factanal(df_entre_z, factors = 1, scores = "regression")
fa_acceptEntre <- factanal(df_acceptEntre_z, factors = 1, scores = "regression")
fa_discrimination <- factanal(df_absDiscrimination_z, factors = 1, scores = "regression")
fa_authority <- factanal(df_challengeAuthority_z, factors = 1, scores = "regression")
df_scores <- df_iso %>%
bind_cols(as.data.frame(fa_cooperation$scores))  %>%
rename(cooperation = Factor1) %>%
bind_cols(as.data.frame(fa_altruism$scores))  %>%
rename(altruism = Factor1) %>%
bind_cols(as.data.frame(fa_workEthics$scores))  %>%
rename(workEthics = Factor1) %>%
bind_cols(as.data.frame(fa_entre$scores))  %>%
rename(entre = Factor1) %>%
bind_cols(as.data.frame(fa_acceptEntre$scores))  %>%
rename(tolerance = Factor1) %>%
bind_cols(as.data.frame(fa_discrimination$scores))  %>%
rename(discrimination = Factor1) %>%
bind_cols(as.data.frame(fa_authority$scores))  %>%
rename(authority = Factor1)
View(df_scores)
dbWriteTable(conn, "vars_AggregateHistoVasHetero_phase2", df_scores, overwrite = TRUE, row.names = FALSE)
dbDisconnect(conn)
library("psych")
library("GPArotation")
library("dplyr")
library(tidyr)
library(RSQLite)
#setwd("F:/Mahdi/Dropbox/Dropbox/code/projectCodes/folktales/scripts_r")
# conn <- dbConnect(SQLite(), "F:/Mahdi/Dropbox/Dropbox/code/projectCodes/folktales/FolktaleDB.sqlite")
conn <- dbConnect(SQLite(), "C:/Users/bax1408/Dropbox/code/projectCodes/folktales/FolktaleDB.sqlite")
# Function to extract loadings and add a label for the source
extract_loadings <- function(factor_analysis, label) {
loadings <- as.data.frame(as.matrix(factor_analysis$loadings)) # Convert loadings to a data frame
loadings$Variable <- rownames(factor_analysis$loadings)       # Add variable names as a column
loadings$Source <- label                                      # Add a column for the source label
rownames(loadings) <- NULL                                    # Reset row names
return(loadings)
}
query_iso <- "select
vars_histoVasHetero_phase2.iso_code,
vars_CrossGeographical.iso_alpha
from vars_histoVasHetero_phase2
left join vars_CrossGeographical on vars_histoVasHetero_phase2.iso_code=vars_CrossGeographical.iso_code
"
df_iso <-  dbGetQuery(conn, query_iso)
query_cooperation <- "select
trust_HHI,
trustWho_HHI,
promise_HHI,
badDeedPunish_HHI,
goodDeedPunish_HHI,
cooperation_HHI,
cooperatePerson_HHI
from
vars_histoVasHetero_phase2"
df_cooperation <- dbGetQuery(conn, query_cooperation)
df_cooperation_z <- as.data.frame(scale(df_cooperation))
query_altruism <- "select
revenge_HHI,
ThirdPartyPunish_HHI,
helpExist_HHI,
helpPerson_HHI
from
vars_histoVasHetero_phase2"
df_altruism <- dbGetQuery(conn, query_altruism)
df_altruism_z <- as.data.frame(scale(df_altruism))
query_workEthics <- "select
honesty_HHI,
attentionDetail_HHI,
hardWork_HHI,
patience_HHI,
punctuality_HHI
from
vars_histoVasHetero_phase2"
df_workEthics <- dbGetQuery(conn, query_workEthics)
df_workEthics_z <- as.data.frame(scale(df_workEthics))
query_entre <- "select
socialStatus_HHI,
geoMobile_HHI,
riskReasonable_HHI,
restart_HHI
from
vars_histoVasHetero_phase2"
df_entre <- dbGetQuery(conn, query_entre)
df_entre_z <- as.data.frame(scale(df_entre))
query_acceptEntre <- "select
individualism_HHI,
differentOutcome_HHI,
diversityAccept_HHI,
changeOpen_HHI,
envy_HHI
from
vars_histoVasHetero_phase2"
df_acceptEntre <- dbGetQuery(conn, query_acceptEntre)
df_acceptEntre_z <- as.data.frame(scale(df_acceptEntre))
query_absDiscrimination <- "select
firstImpress_HHI,
femaleSaved_HHI,
elderlyTreat_HHI,
discriminationEnco_HHI,
discriminationFemaleEnco_HHI
from
vars_histoVasHetero_phase2"
df_absDiscrimination <- dbGetQuery(conn, query_absDiscrimination)
df_absDiscrimination_z <- as.data.frame(scale(df_absDiscrimination))
query_challengeAuthority <- "select
normAbidance_HHI,
ChallengeAuthority_HHI,
challengedAuthorityType_HHI
from
vars_histoVasHetero_phase2
"
df_challengeAuthority <- dbGetQuery(conn, query_challengeAuthority)
df_challengeAuthority_z <- as.data.frame(scale(df_challengeAuthority))
fa_cooperation <- factanal(df_cooperation_z, factors = 1, scores = "regression")
fa_altruism <- factanal(df_altruism_z, factors = 1, scores = "regression")
fa_workEthics <- factanal(df_workEthics_z, factors = 1, scores = "regression")
fa_entre <- factanal(df_entre_z, factors = 1, scores = "regression")
fa_acceptEntre <- factanal(df_acceptEntre_z, factors = 1, scores = "regression")
fa_discrimination <- factanal(df_absDiscrimination_z, factors = 1, scores = "regression")
fa_authority <- factanal(df_challengeAuthority_z, factors = 1, scores = "regression")
df_scores <- df_iso %>%
bind_cols(as.data.frame(fa_cooperation$scores))  %>%
rename(cooperation = Factor1) %>%
bind_cols(as.data.frame(fa_altruism$scores))  %>%
rename(altruism = Factor1) %>%
bind_cols(as.data.frame(fa_workEthics$scores))  %>%
rename(workEthic = Factor1) %>%
bind_cols(as.data.frame(fa_entre$scores))  %>%
rename(entre = Factor1) %>%
bind_cols(as.data.frame(fa_acceptEntre$scores))  %>%
rename(tolerance = Factor1) %>%
bind_cols(as.data.frame(fa_discrimination$scores))  %>%
rename(discrimination = Factor1) %>%
bind_cols(as.data.frame(fa_authority$scores))  %>%
rename(authority = Factor1)
dbWriteTable(conn, "vars_AggregateHistoVasHetero_phase2", df_scores, overwrite = TRUE, row.names = FALSE)
dbDisconnect(conn)
knitr::opts_chunk$set(echo = TRUE)
# List of packages for text analysis
utils::install.packages("tm",           # Text mining
"SnowballC",    # Word stemming
"wordcloud",    # Word clouds
"textclean",    # Cleaning text
"textdata",     # Pretrained sentiment lexicons
"tidytext",     # Tidy text analysis
"tokenizers",   # Tokenizing text
"quanteda",     # Advanced text analysis
"stringr",      # String manipulation
"text",         # Embedding models and more
"syuzhet",      # Sentiment analysis
"readtext",     # Reading text files
"topicmodels",  # Topic modeling
"ggplot2"       # Visualization)
)
# List of packages for text analysis
packages <- c("tm",           # Text mining
"SnowballC",    # Word stemming
"wordcloud",    # Word clouds
"textclean",    # Cleaning text
"textdata",     # Pretrained sentiment lexicons
"tidytext",     # Tidy text analysis
"tokenizers",   # Tokenizing text
"quanteda",     # Advanced text analysis
"stringr",      # String manipulation
"text",         # Embedding models and more
"syuzhet",      # Sentiment analysis
"readtext",     # Reading text files
"topicmodels",  # Topic modeling
"ggplot2"       # Visualization
)
# Install only those not yet installed
installed <- rownames(installed.packages())
to_install <- setdiff(packages, installed)
if (length(to_install) > 0) {
install.packages(to_install)
}
print("helo world")
print("helo world")
print("helo world")
printsdf("helo world")
install.packages("countdown", repos = "https://cloud.r-project.org")
ajsdf√∂la
knitr::opts_chunk$set(echo = TRUE)
vec_num <- c(1,2,3,4,5)
print(vec_num)
vec_char <-  c("law", "economics", "psychology", "biology", "cognitive science")
print(vec_char)
print(vec_char[0])
print(vec_char[1])
vec_num2 <- vec_num * 2
print(vec_num2)
vec_num2 <- vec_num ** 2
print(vec_num2)
vec_logi_gt5 <-  vec_num2 >= 5
print(vec_logi_gt5)
vec_char2 <- paste(c("red", "yellow", "orange", "green"), vec_char)
print(vec_char2)
names(vec_num) <- vec_char
print(vec_num)
vec1 = c("1980", "1990", "2000", "2010", "2020")
vec2 = c("England", "US", "Germany", "France", "Italy")
dat_fruit <- data.frame(name = vec2, count = vec1)
print(dat_fruit)
print(nrow(dat_fruit), ncol(dat_fruit))
print(nrow(dat_fruit))
print(ncol(dat_fruit))
knitr::opts_chunk$set(echo = TRUE)
# Load required packages
library("readtext")
library("tm")
library("SnowballC")
library("wordcloud")
library("textclean")
library("textdata")
library("tidytext")
library("tokenizers")
library("quanteda")
library("quanteda.textmodels")
library("quanteda.textplots")
library("quanteda.textstats")
library("textstem")
library("stringr")
library("syuzhet")
library("topicmodels")
library("ggplot2")
library("countdown")
library("tidyr")
library("dplyr")
# make sure that you set the directory correctly
setwd("C:/Users/bax1408/Dropbox/code/courseTeach/Computational Social Science/_R/s1")
# setwd("F:/Mahdi/Dropbox/Dropbox/code/courseTeach/computationalSocialScience/_R/s1")
# make sure that you set the directory correctly
setwd("C:/Users/bax1408/Dropbox/code/courseTeach/Computational Social Science/_R/s1")
# setwd("F:/Mahdi/Dropbox/Dropbox/code/courseTeach/computationalSocialScience/_R/s1")
setwd("C:/Users/bax1408/Dropbox/code/courseTeach/computationalSocialScience/_R/s1")
# make sure that you set the directory correctly
print(getwd())
# The directory
fileAddress <- '../../corpus/examples/unSpeeches2000'
# Read all .txt files in the directory
dfPreamble <- readtext(paste0(fileAddress, "/*.txt"),
docvarsfrom = "filenames",
docvarnames = c("isoAlpha", "session", "year"),
dvsep = "_",
encoding = "ISO-8859-1")
View(dfPreamble)
head(dfPreamble)
# The directory
fileAddress <- '../../corpus/examples/unSpeeches2000'
# Read all .txt files in the directory
dfPreamble <- readtext(paste0(fileAddress, "/*.txt"),
docvarsfrom = "filenames",
docvarnames = c("isoAlpha", "session", "year"),
dvsep = "_",
encoding = "ISO-8859-1")
head(dfPreamble)
corpusPreamble <- corpus(dfPreamble)
print(corpusPreamble)
print(corpusPreamble)
summary(corpusPreamble)
summary(corpusPreamble, 10)
summary(corpusPreamble, 20)
table(corpusPreamble&isoAlpha)
View(dfPreamble)
table(corpusPreamble$isoAlpha)
print(as.character(corpusPreamble[1]))
textstat_summary(corpusPreamble)
epcomplexity <- textstat_readability(corpusPreamble, c("Flesch","LIW"))
tokenPreamble <- tokens(corpusPreamble)
# Remove general stopwords
tokenPreambleClean <- tokens(
corpusPreamble,
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE,
remove_url = TRUE
) %>%
tokens_remove(stopwords("english"))
tokenPreambleCleanStemmed <- tokenPreambleClean %>%
tokens_wordstem(language = "english")
install.packages("readxl", lib="C:/Users/bax1408/AppData/Local/Programs/R/R-4.4.1/library")
# Load required packages
library("readtext")
library("tm")
library("SnowballC")
library("wordcloud")
library("textclean")
library("textdata")
library("tidytext")
library("tokenizers")
library("quanteda")
library("quanteda.textmodels")
library("quanteda.textplots")
library("quanteda.textstats")
library("textstem")
library("stringr")
library("syuzhet")
library("topicmodels")
library("ggplot2")
###
library("countdown")
library("readxl")
library("tidyr")
library("dplyr")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
excel_data <- read_excel(fileAddress2, sheet = "Sheet1")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2, sheet = "Sheet1")
# The directory
fileAddress1 <- '../../corpus/examples/unSpeeches2000_2010'
# Read all .txt files in the directory
dfUNSpeech <- readtext(paste0(fileAddress1, "/*.txt"),
docvarsfrom = "filenames",
docvarnames = c("isoAlpha", "session", "year"),
dvsep = "_",
encoding = "ISO-8859-1")
View(dfUNSpeech)
View(speakerData)
# The directory
fileAddress1 <- '../../corpus/examples/unSpeeches2000_2010'
# Read all .txt files in the directory
dfUNSpeech <- readtext(paste0(fileAddress1, "/*.txt"),
docvarsfrom = "filenames",
docvarnames = c("isoAlpha", "session", "year"),
dvsep = "_",
encoding = "ISO-8859-1")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
merged_data <- inner_join(dfUNSpeech, speakerData, by = "isoAlpha")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = "isoAlpha")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
View(speakerData)
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
View(dfUNSpeechComplete)
head(dfPreamble)
corpusUNSpeechComplete <- corpus(dfUNSpeechComplete)
print(corpusUNSpeechComplete)
summary(corpusPreamble, 20)
# The directory
fileAddress1 <- '../../corpus/examples/unSpeeches2000_2010'
# Read all .txt files in the directory
dfUNSpeech <- readtext(paste0(fileAddress1, "/*.txt"),
docvarsfrom = "filenames",
docvarnames = c("isoAlpha", "session", "year"),
dvsep = "_",
encoding = "ISO-8859-1")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
corpusUNSpeechComplete <- corpus(dfUNSpeechComplete)
print(corpusUNSpeechComplete)
table(corpusPreamble$post)
summary(corpusUNSpeechComplete, 20)
table(corpusUNSpeechComplete$post)
textstat_summary(corpusUNSpeechComplete)
textstat_summary(corpusUNSpeechComplete$post)
textstat_summary(corpusUNSpeechComplete)
epcomplexity <- textstat_readability(corpusPreamble, c("Flesch","LIW"))
View(epcomplexity)
epcomplexity <- textstat_readability(corpusUNSpeechComplete, c("Flesch","LIW"))
View(epcomplexity)
View(epcomplexity)
epcomplexity <- textstat_readability(corpusUNSpeechComplete, c("Flesch","LIW"))
epcomplexity <- data.frame(epcomplexity,post=corpusUNSpeechComplete$post)
View(epcomplexity)
epcomplexity %>%
ggplot(aes(x=fct_reorder(post,Flesch), y=Flesch,fill=post)) +
geom_boxplot() +
geom_jitter(position=position_jitter(width=.1, height=0))+
theme_bw()+
labs(x="",y="Flesch Reading Ease Score")+
theme(legend.position = "none")
# Load required packages
library("readtext")
library("tm")
library("SnowballC")
library("wordcloud")
library("textclean")
library("textdata")
library("tidytext")
library("tokenizers")
library("quanteda")
library("quanteda.textmodels")
library("quanteda.textplots")
library("quanteda.textstats")
library("textstem")
library("stringr")
library("syuzhet")
library("topicmodels")
library("ggplot2")
###
library("countdown")
library("readxl")
library("tidyr")
library("dplyr")
library("tidyverse")
install.packages("tidyverse", lib="C:/Users/bax1408/AppData/Local/Programs/R/R-4.4.1/library")
# Load required packages
library("readtext")
library("tm")
library("SnowballC")
library("wordcloud")
library("textclean")
library("textdata")
library("tidytext")
library("tokenizers")
library("quanteda")
library("quanteda.textmodels")
library("quanteda.textplots")
library("quanteda.textstats")
library("textstem")
library("stringr")
library("syuzhet")
library("topicmodels")
library("ggplot2")
###
library("countdown")
library("readxl")
library("tidyr")
library("dplyr")
library("tidyverse")
epcomplexity %>%
ggplot(aes(x=fct_reorder(post,Flesch), y=Flesch,fill=post)) +
geom_boxplot() +
geom_jitter(position=position_jitter(width=.1, height=0))+
theme_bw()+
labs(x="",y="Flesch Reading Ease Score")+
theme(legend.position = "none")
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
# Read your file
fileAddress2 = '../../corpus/examples/speakersSession.xlsx'
speakerData <- read_excel(fileAddress2)
dfUNSpeechComplete <- inner_join(dfUNSpeech, speakerData, by = c("isoAlpha", 'year'))
corpusUNSpeechComplete <- corpus(dfUNSpeechComplete)
print(corpusUNSpeechComplete)
summary(corpusUNSpeechComplete, 20)
table(corpusUNSpeechComplete$post)
print(as.character(corpusPreamble[1]))
textstat_summary(corpusUNSpeechComplete)
epcomplexity <- textstat_readability(corpusUNSpeechComplete, c("Flesch","LIW"))
epcomplexity <- data.frame(epcomplexity,post=corpusUNSpeechComplete$post)
epcomplexity %>%
ggplot(aes(x=fct_reorder(post,Flesch), y=Flesch,fill=post)) +
geom_boxplot() +
geom_jitter(position=position_jitter(width=.1, height=0))+
theme_bw()+
labs(x="",y="Flesch Reading Ease Score")+
theme(legend.position = "none")
